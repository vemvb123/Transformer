# Transformer

The project is explained and detailed in the pdf report.

<img width="521" height="723" alt="image" src="https://github.com/user-attachments/assets/194b847e-8745-4ea4-be45-491168efb0ae" />


This paper showcases the development of a transformer. This is an interest-
ing project because it shows how a transformer works, by showcasing code
and results for the transformer. 

It handles sequence data and uses self-attention mechanisms to determine
weights and spot long-range dependencies. The architecture is widely used.
It is used as a building block in language models like ChatGPT and BERT

