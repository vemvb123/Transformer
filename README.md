# Transformer

The project is explained and detailed in the pdf report.

<img width="521" height="723" alt="image" src="https://github.com/user-attachments/assets/194b847e-8745-4ea4-be45-491168efb0ae" />


This paper showcases the development of a transformer with JAX.
This is an intresting project because it shows how a transformer works, by showcasing code and results for the transformer. 

A transformer handles sequence data and uses self-attention mechanisms to determine
weights and spot long-range dependencies. The architecture is widely used.
It is used as a building block in language models like ChatGPT and BERT

# How to run
The project can be run from the train.py file.
After training, a weights will be saved in a .pkl file.
The model can be adjusted in the config.py file.



